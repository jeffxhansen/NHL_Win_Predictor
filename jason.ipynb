{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonvasquez/opt/anaconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from py_files import data_manager\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pgmpy.models import BayesianNetwork, BayesianModel\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, ExpectationMaximization\n",
    "from pgmpy.inference import VariableElimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/54/zvdnrgtn2fb_bdky24ktf8jc0000gn/T/ipykernel_19806/356840926.py:2: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/play_by_play_2018_19.csv', encoding='latin1')\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('data/play_by_play_2018_19.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonvasquez/opt/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/jasonvasquez/opt/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xg</th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event</th>\n",
       "      <th>secondary_type</th>\n",
       "      <th>event_team</th>\n",
       "      <th>event_team_type</th>\n",
       "      <th>description</th>\n",
       "      <th>period</th>\n",
       "      <th>period_seconds</th>\n",
       "      <th>...</th>\n",
       "      <th>home_conference_name</th>\n",
       "      <th>home_id</th>\n",
       "      <th>away_name</th>\n",
       "      <th>away_abbreviation</th>\n",
       "      <th>away_division_name</th>\n",
       "      <th>away_division_name_short</th>\n",
       "      <th>away_conference_name</th>\n",
       "      <th>away_id</th>\n",
       "      <th>venue_id</th>\n",
       "      <th>team_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.018020e+13</td>\n",
       "      <td>CHANGE</td>\n",
       "      <td>Change</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montréal Canadiens</td>\n",
       "      <td>away</td>\n",
       "      <td>ON: Carey Price, Jeff Petry, Paul Byron, Arttu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>10</td>\n",
       "      <td>Montréal Canadiens</td>\n",
       "      <td>MTL</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.018020e+13</td>\n",
       "      <td>CHANGE</td>\n",
       "      <td>Change</td>\n",
       "      <td>Line change</td>\n",
       "      <td>Toronto Maple Leafs</td>\n",
       "      <td>home</td>\n",
       "      <td>ON: Patrick Marleau, Jake Gardiner, Tyler Enni...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>10</td>\n",
       "      <td>Montréal Canadiens</td>\n",
       "      <td>MTL</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.018020e+13</td>\n",
       "      <td>FACEOFF</td>\n",
       "      <td>Faceoff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montréal Canadiens</td>\n",
       "      <td>away</td>\n",
       "      <td>Max Domi faceoff won against Auston Matthews</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>10</td>\n",
       "      <td>Montréal Canadiens</td>\n",
       "      <td>MTL</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019187</td>\n",
       "      <td>2.018020e+13</td>\n",
       "      <td>SHOT</td>\n",
       "      <td>Shot</td>\n",
       "      <td>Backhand</td>\n",
       "      <td>Montréal Canadiens</td>\n",
       "      <td>away</td>\n",
       "      <td>Artturi Lehkonen Backhand saved by Frederik An...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>10</td>\n",
       "      <td>Montréal Canadiens</td>\n",
       "      <td>MTL</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.018020e+13</td>\n",
       "      <td>CHANGE</td>\n",
       "      <td>Change</td>\n",
       "      <td>On the fly</td>\n",
       "      <td>Montréal Canadiens</td>\n",
       "      <td>away</td>\n",
       "      <td>ON: Tomas Tatar, Brendan Gallagher; OFF: Paul ...</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>10</td>\n",
       "      <td>Montréal Canadiens</td>\n",
       "      <td>MTL</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         xg      event_id event_type    event secondary_type  \\\n",
       "1       NaN  2.018020e+13     CHANGE   Change            NaN   \n",
       "2       NaN  2.018020e+13     CHANGE   Change    Line change   \n",
       "3       NaN  2.018020e+13    FACEOFF  Faceoff            NaN   \n",
       "4  0.019187  2.018020e+13       SHOT     Shot       Backhand   \n",
       "5       NaN  2.018020e+13     CHANGE   Change     On the fly   \n",
       "\n",
       "            event_team event_team_type  \\\n",
       "1   Montréal Canadiens            away   \n",
       "2  Toronto Maple Leafs            home   \n",
       "3   Montréal Canadiens            away   \n",
       "4   Montréal Canadiens            away   \n",
       "5   Montréal Canadiens            away   \n",
       "\n",
       "                                         description  period  period_seconds  \\\n",
       "1  ON: Carey Price, Jeff Petry, Paul Byron, Arttu...       1               0   \n",
       "2  ON: Patrick Marleau, Jake Gardiner, Tyler Enni...       1               0   \n",
       "3       Max Domi faceoff won against Auston Matthews       1               0   \n",
       "4  Artturi Lehkonen Backhand saved by Frederik An...       1              29   \n",
       "5  ON: Tomas Tatar, Brendan Gallagher; OFF: Paul ...       1              38   \n",
       "\n",
       "   ...  home_conference_name  home_id           away_name  away_abbreviation  \\\n",
       "1  ...               Eastern       10  Montréal Canadiens                MTL   \n",
       "2  ...               Eastern       10  Montréal Canadiens                MTL   \n",
       "3  ...               Eastern       10  Montréal Canadiens                MTL   \n",
       "4  ...               Eastern       10  Montréal Canadiens                MTL   \n",
       "5  ...               Eastern       10  Montréal Canadiens                MTL   \n",
       "\n",
       "   away_division_name away_division_name_short away_conference_name away_id  \\\n",
       "1            Atlantic                      ATL              Eastern       8   \n",
       "2            Atlantic                      ATL              Eastern       8   \n",
       "3            Atlantic                      ATL              Eastern       8   \n",
       "4            Atlantic                      ATL              Eastern       8   \n",
       "5            Atlantic                      ATL              Eastern       8   \n",
       "\n",
       "  venue_id team_encoded  \n",
       "1      NaN           15  \n",
       "2      NaN           26  \n",
       "3      NaN           15  \n",
       "4      NaN           15  \n",
       "5      NaN           15  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['event_team_type'])\n",
    "    \n",
    "# Get all unique game ids\n",
    "unique_game_ids = df.game_id.unique()\n",
    "\n",
    "# Get the label encoder for the teams\n",
    "team_names = np.sort(df.event_team.dropna().unique())\n",
    "label_encoder = data_manager.get_label_encoder(team_names)\n",
    "\n",
    "# Label encode the teams\n",
    "df['team_encoded'] = label_encoder.transform(df.event_team)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df, game_id):\n",
    "    \"\"\"\n",
    "    Extracts and organizes event data for a specific game.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame containing the game data.\n",
    "    game_id (int): The ID of the game to retrieve data for.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two DataFrames, one for home team events and one for away team events.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the DataFrame to get data for the specified game and drop rows with NaN values in event_team_type\n",
    "    game1 = df[df.game_id == game_id] \n",
    "    game1 = game1.dropna(subset=['event_team_type'])\n",
    "\n",
    "    # Get unique event types (excluding CHANGE)\n",
    "    event_types = game1.event_type.unique()\n",
    "    event_types = np.delete(event_types, np.where(event_types == 'CHANGE'))\n",
    "    event_types = np.append(event_types, ['TIME_REMAINING', 'HOME', 'WIN', 'TEAM', 'GAME_ID', 'CORSI', 'FENWICK'])\n",
    "\n",
    "    # Create dictionaries to store event counts for home and away teams\n",
    "    final_dict = {f'HOME_{event}': 0 for event in event_types}\n",
    "    away_dict = {f'AWAY_{event}': 0 for event in event_types}\n",
    "    final_dict.update(away_dict)\n",
    "\n",
    "    # Create DataFrames to store event data for home and away teams\n",
    "    final_df = False\n",
    "\n",
    "    # Iterate through the events in the game and count them\n",
    "    for _, row in game1.iterrows():\n",
    "        # Skip events with NaN event_team_type or events of type CHANGE\n",
    "        if pd.isnull(row['event_team_type']) or row['event_type'] == 'CHANGE':\n",
    "            continue\n",
    "        \n",
    "        # Determine if the event belongs to the home or away team and update counts accordingly\n",
    "        if row['event_team_type'] == 'home':\n",
    "            final_dict[f\"HOME_{row['event_type']}\"] += 1\n",
    "            final_dict['TIME_REMAINING'] = row['game_seconds_remaining']\n",
    "            final_dict['HOME_HOME'] = 1\n",
    "            final_dict['WIN'] = 1 if row['home_final'] > row['away_final'] else 0\n",
    "            final_dict['HOME_TEAM'] = row['team_encoded']\n",
    "            final_dict['GAME_ID'] = game_id\n",
    "\n",
    "            # Get the Corsi and Fenwick for the home team\n",
    "            if row['strength_code'] == \"EV\":\n",
    "                final_dict['HOME_CORSI'] = calculate_corsi(final_dict, 'HOME')\n",
    "                final_dict['HOME_FENWICK'] = calculate_fenwick(final_dict, 'HOME')\n",
    "                final_dict['HOME_CORSI_FOR'] = calculate_corsi_for(final_dict['HOME_CORSI'], final_dict['AWAY_CORSI'])\n",
    "                final_dict['HOME_FENWICK_FOR'] = calculate_fenwick_for(final_dict['HOME_FENWICK'], final_dict['AWAY_FENWICK'])\n",
    "            \n",
    "            # home_df = home_df.append(home_dict, ignore_index=True)\n",
    "        else:\n",
    "            final_dict[f\"AWAY_{row['event_type']}\"] += 1\n",
    "            final_dict['TIME_REMAINING'] = row['game_seconds_remaining']\n",
    "            final_dict['AWAY_HOME'] = 0\n",
    "            final_dict['WIN'] = 0 if row['home_final'] < row['away_final'] else 1\n",
    "            final_dict['AWAY_TEAM'] = row['team_encoded']\n",
    "            final_dict['GAME_ID'] = game_id\n",
    "            \n",
    "            # Get the Corsi and Fenwick for the away team\n",
    "            if row['strength_code'] == \"EV\":\n",
    "                final_dict['AWAY_CORSI'] = calculate_corsi(final_dict, \"AWAY\")\n",
    "                final_dict['AWAY_FENWICK'] = calculate_fenwick(final_dict, \"AWAY\")\n",
    "                final_dict['AWAY_CORSI_FOR'] = calculate_corsi_for(final_dict['AWAY_CORSI'], final_dict['HOME_CORSI'])\n",
    "                final_dict['AWAY_FENWICK_FOR'] = calculate_fenwick_for(final_dict['AWAY_FENWICK'], final_dict['HOME_FENWICK'])\n",
    "                \n",
    "            # away_df = away_df.append(away_dict, ignore_index=True)\n",
    "        if type(final_df) is bool:\n",
    "            # print(final_dict)\n",
    "            final_df = pd.DataFrame(final_dict, index=[0])\n",
    "        else:\n",
    "            append_row = pd.DataFrame(final_dict, index=[0])\n",
    "            final_df = pd.concat([final_df, append_row], ignore_index=True)\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "def get_label_encoder(teams):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    le.fit(teams)\n",
    "    return le\n",
    "\n",
    "\n",
    "def calculate_corsi(team_dict, h_or_a):\n",
    "    \"\"\"\n",
    "    Calculates the Corsi for a team.\n",
    "\n",
    "    Args:\n",
    "    team_dict (dict): A dictionary containing event counts for a team.\n",
    "\n",
    "    Returns:\n",
    "    int: The Corsi for the team.\n",
    "    \"\"\"\n",
    "    return team_dict[f'{h_or_a}_SHOT'] + team_dict[f'{h_or_a}_MISSED_SHOT'] + team_dict[f'{h_or_a}_BLOCKED_SHOT']\n",
    "\n",
    "\n",
    "def calculate_fenwick(team_dict, h_or_a):\n",
    "    \"\"\"\n",
    "    Calculates the Fenwick for a team.\n",
    "\n",
    "    Args:\n",
    "    team_dict (dict): A dictionary containing event counts for a team.\n",
    "\n",
    "    Returns:\n",
    "    int: The Fenwick for the team.\n",
    "    \"\"\"\n",
    "    return team_dict[f'{h_or_a}_SHOT'] + team_dict[f'{h_or_a}_MISSED_SHOT']\n",
    "\n",
    "\n",
    "def calculate_corsi_for(team_corsi, opp_corsi):\n",
    "    \"\"\"\n",
    "    Calculates the Corsi For Percentage for a team.\n",
    "\n",
    "    Args:\n",
    "    team_corsi (int): The Corsi for the team.\n",
    "    opp_corsi (int): The Corsi for the opposing team.\n",
    "\n",
    "    Returns:\n",
    "    float: The Corsi For Percentage for the team.\n",
    "    \"\"\"\n",
    "    # Check if the denominator is zero to avoid division by zero\n",
    "    if team_corsi + opp_corsi == 0:\n",
    "        return 0\n",
    "    \n",
    "    return team_corsi / (team_corsi + opp_corsi)\n",
    "\n",
    "\n",
    "def calculate_fenwick_for(team_fenwick, opp_fenwick):\n",
    "    \"\"\"\n",
    "    Calculates the Fenwick For Percentage for a team.\n",
    "\n",
    "    Args:\n",
    "    team_fenwick (int): The Fenwick for the team.\n",
    "    opp_fenwick (int): The Fenwick for the opposing team.\n",
    "\n",
    "    Returns:\n",
    "    float: The Fenwick For Percentage for the team.\n",
    "    \"\"\"\n",
    "    # Check if the denominator is zero to avoid division by zero\n",
    "    if team_fenwick + opp_fenwick == 0:\n",
    "        return 0\n",
    "    \n",
    "    return team_fenwick / (team_fenwick + opp_fenwick)\n",
    "\n",
    "def merge_home_away(home_df, away_df):\n",
    "     # change the names in the home_df to be HOME_[current column name] and same for away_df\n",
    "     home_df.columns = ['HOME_' + col for col in home_df.columns]\n",
    "     away_df.columns = ['AWAY_' + col for col in away_df.columns]\n",
    "     # change TIME_REMAINING, WIN, GAME_ID to be the same in both dataframes\n",
    "     home_df = home_df.rename(columns={'HOME_TIME_REMAINING': 'TIME_REMAINING', 'HOME_WIN': 'WIN', 'HOME_GAME_ID': 'GAME_ID'})\n",
    "     away_df = away_df.rename(columns={'AWAY_TIME_REMAINING': 'TIME_REMAINING', 'AWAY_WIN': 'WIN', 'AWAY_GAME_ID': 'GAME_ID'})\n",
    "\n",
    "     full = pd.merge(home_df, away_df, on=['TIME_REMAINING', 'WIN', 'GAME_ID'], how='outer')\n",
    "\n",
    "     # if home_df has 1 in WIN column than make all the values in the WIN column 1\n",
    "     if 1 in home_df.WIN.unique():\n",
    "          full.WIN = 1\n",
    "     elif 1 in away_df.WIN.unique():\n",
    "          full.WIN = 0\n",
    "\n",
    "     full.drop(columns=['HOME_HOME', 'AWAY_HOME'])\n",
    "     full = full.sort_values(by='TIME_REMAINING')\n",
    "     full.fillna(method='ffill', inplace=True)\n",
    "     full.fillna(0, inplace=True)\n",
    "\n",
    "     return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonvasquez/opt/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/jasonvasquez/opt/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "100%|██████████| 1358/1358 [07:56<00:00,  2.44it/s]"
     ]
    }
   ],
   "source": [
    "# Copy the dataframe\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Drop NaN's from event_team_type\n",
    "df_copy = df_copy.dropna(subset=['event_team'])\n",
    "\n",
    "# Get all of the data\n",
    "unique_game_ids = df.game_id.unique()\n",
    "\n",
    "# Get the label encoder for the teams\n",
    "team_names = np.sort(df_copy.event_team.dropna().unique())\n",
    "label_encoder = get_label_encoder(team_names)\n",
    "\n",
    "# Label encode the teams\n",
    "df_copy['team_encoded'] = label_encoder.transform(df_copy.event_team)\n",
    "\n",
    "# Iterate through the games\n",
    "game_bar = tqdm(total=len(unique_game_ids))\n",
    "final_df = pd.DataFrame()\n",
    "for game_id in unique_game_ids:\n",
    "    final_df_p = get_data(df_copy, game_id)\n",
    "    final_df = pd.concat([final_df, final_df_p], ignore_index=True)\n",
    "    game_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'WIN'\n",
    "df = final_df.copy()\n",
    "\n",
    "X = df.drop(columns=[label_column, 'HOME_GAME_ID', 'AWAY_GAME_ID', 'HOME_TIME_REMAINING', 'AWAY_TIME_REMAINING', 'HOME_WIN', 'AWAY_WIN'])\n",
    "y = df[label_column]\n",
    "\n",
    "game_groups = df.groupby('GAME_ID')\n",
    "\n",
    "game_ids = df.GAME_ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "endog is required to have ndim 1 but has ndim 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget variable y must contain binary classes (0 and 1)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Instantiate and train the exponential smoothing model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mExponentialSmoothing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamped_trend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m fitted_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/util/_decorators.py:213\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    212\u001b[0m     kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/statsmodels/tsa/holtwinters/model.py:227\u001b[0m, in \u001b[0;36mExponentialSmoothing.__init__\u001b[0;34m(self, endog, trend, damped_trend, seasonal, seasonal_periods, initialization_method, initial_level, initial_trend, initial_seasonal, use_boxcox, bounds, dates, freq, missing)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdamped\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdamped_trend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m ):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, \u001b[38;5;28;01mNone\u001b[39;00m, dates, freq, missing\u001b[38;5;241m=\u001b[39mmissing)\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[43marray_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     options \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiplicative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m     trend \u001b[38;5;241m=\u001b[39m string_like(trend, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrend\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m=\u001b[39moptions, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/statsmodels/tools/validation/validation.py:155\u001b[0m, in \u001b[0;36marray_like\u001b[0;34m(obj, name, dtype, ndim, maxdim, shape, order, contiguous, optional, writeable)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m ndim:\n\u001b[1;32m    154\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is required to have ndim \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m but has ndim \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(name, ndim, arr\u001b[38;5;241m.\u001b[39mndim))\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m actual, req \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape, shape):\n",
      "\u001b[0;31mValueError\u001b[0m: endog is required to have ndim 1 but has ndim 2"
     ]
    }
   ],
   "source": [
    "# Define X and y (features and target)\n",
    "X = df.drop(['WIN', 'GAME_ID'], axis=1)  # Assuming 'WIN' is the target variable\n",
    "y = df['WIN']  # Keep y as a pandas Series\n",
    "\n",
    "# Ensure y contains only binary classes (0 and 1)\n",
    "y_unique = y.unique()\n",
    "if len(y_unique) != 2 or 0 not in y_unique or 1 not in y_unique:\n",
    "    raise ValueError(\"Target variable y must contain binary classes (0 and 1)\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "\n",
    "# Instantiate and train the exponential smoothing model\n",
    "model = ExponentialSmoothing(X, trend='add', damped_trend=True)\n",
    "fitted_model = model.fit()\n",
    "break\n",
    "\n",
    "# Extract data for a certain timestep from your dataset\n",
    "game_id = df['GAME_ID'].unique()[2]  # Specify the game ID\n",
    "desired_timestep = 1500  # Specify the desired timestep\n",
    "game_data = df[df['GAME_ID'] == game_id]\n",
    "timestep_data = game_data[game_data['TIME_REMAINING'] < desired_timestep]\n",
    "\n",
    "# Prepare the timestep data for prediction\n",
    "X_timestep = timestep_data.drop(['WIN', 'GAME_ID'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Predict the outcome for the timestep\n",
    "outcome = fitted_model.predict(start=len(y), end=len(y) + len(X_timestep) - 1)\n",
    "\n",
    "# Assuming binary outcome (0 or 1)\n",
    "outcome_binary = (outcome > 0.5).astype(int)\n",
    "\n",
    "# get final prediction of outcome binary\n",
    "\n",
    "# Print the predicted outcome\n",
    "print(\"Predicted outcome:\", 'Home team win')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_hdf('data/processed_data.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_hdf('data/processed_data.h5', key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'WIN'\n",
    "df = final_df.copy()\n",
    "\n",
    "X = df.drop(columns=[label_column, 'HOME_GAME_ID', 'AWAY_GAME_ID', 'HOME_TIME_REMAINING', 'AWAY_TIME_REMAINING'])\n",
    "y = df[label_column]\n",
    "\n",
    "game_groups = df.groupby('GAME_ID')\n",
    "\n",
    "game_ids = df.GAME_ID.unique()\n",
    "\n",
    "train_ids, test_ids = train_test_split(game_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train = df[df.GAME_ID.isin(train_ids)]\n",
    "test = df[df.GAME_ID.isin(test_ids)]\n",
    "\n",
    "train_X = train.drop(columns=[label_column])\n",
    "train_y = train[label_column]\n",
    "test_X = test.drop(columns=[label_column])\n",
    "test_y = test[label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepers = [\n",
    "    \"HOME_GOAL\",\n",
    "    \"AWAY_GOAL\",\n",
    "    \"HOME_SHOT\",\n",
    "    \"AWAY_SHOT\",\n",
    "    \"HOME_MISSED_SHOT\",\n",
    "    \"AWAY_MISSED_SHOT\",\n",
    "    \"HOME_BLOCKED_SHOT\",\n",
    "    \"AWAY_BLOCKED_SHOT\",\n",
    "    \"HOME_FACEOFF\",\n",
    "    \"AWAY_FACEOFF\",\n",
    "    \"TIME_REMAINING\",\n",
    "    \"WIN\",\n",
    "]\n",
    "\n",
    "train = train[keepers]\n",
    "test = test[keepers]\n",
    "\n",
    "# take 10% of train\n",
    "train2 = train.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [02:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Home Team Win: 0.5413602941176471\n",
      "Probability of Away Team Win: 0.4586397058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "keepers = [\n",
    "    \"HOME_GOAL\",\n",
    "    \"AWAY_GOAL\",\n",
    "    \"TIME_REMAINING\",\n",
    "    \"WIN\",\n",
    "]\n",
    "\n",
    "train = train[keepers]\n",
    "test = test[keepers]\n",
    "\n",
    "# take 10% of train\n",
    "train2 = train\n",
    "\n",
    "model_dict = [(feature, 'WIN') for feature in train.columns if feature != 'WIN']\n",
    "\n",
    "# Define the structure of the Bayesian network\n",
    "model = BayesianNetwork(model_dict)\n",
    "\n",
    "# Learn the parameters of the Bayesian network using Maximum Likelihood Estimation\n",
    "model.fit(train2, estimator=ExpectationMaximization)\n",
    "\n",
    "# Create an inference object for probabilistic inference\n",
    "inference = VariableElimination(model)\n",
    "\n",
    "evidence = {\n",
    "    'HOME_GOAL': 0,\n",
    "    'AWAY_GOAL': 0,\n",
    "    'TIME_REMAINING': 3600\n",
    "}\n",
    "\n",
    "# Perform inference to compute posterior probabilities\n",
    "posterior_probabilities = inference.query(variables=['WIN'], evidence=evidence)\n",
    "\n",
    "# Extract probabilities of each outcome\n",
    "home_team_win_probability = posterior_probabilities.values[1]\n",
    "away_team_win_probability = posterior_probabilities.values[0]\n",
    "\n",
    "print(\"Probability of Home Team Win:\", home_team_win_probability)\n",
    "print(\"Probability of Away Team Win:\", away_team_win_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Home Team Win: 0.8174991249864597\n",
      "Probability of Away Team Win: 0.18250087501354026\n"
     ]
    }
   ],
   "source": [
    "inference = VariableElimination(model)\n",
    "\n",
    "evidence = {\n",
    "    'HOME_GOAL': 3,\n",
    "    'AWAY_GOAL': 0,\n",
    "}\n",
    "\n",
    "# Perform inference to compute posterior probabilities\n",
    "posterior_probabilities = inference.query(variables=['WIN'], evidence=evidence)\n",
    "\n",
    "# Extract probabilities of each outcome\n",
    "home_team_win_probability = posterior_probabilities.values[1]\n",
    "away_team_win_probability = posterior_probabilities.values[0]\n",
    "\n",
    "print(\"Probability of Home Team Win:\", home_team_win_probability)\n",
    "print(\"Probability of Away Team Win:\", away_team_win_probability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

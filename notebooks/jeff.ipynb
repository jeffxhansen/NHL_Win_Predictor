{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jeffutils.utils import set_np_pd_display_params, print_display, movecol\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from random import choice\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import pickle\n",
    "from scipy.stats import entropy, wasserstein_distance, pearsonr\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "from py_files.data_manager import RELEVANT_COLUMNS, EVENTS_IGNORED, load_and_clean_csv\n",
    "from py_files.helper_funcs import get_data, get_label_encoder\n",
    "\n",
    "set_np_pd_display_params(np, pd)\n",
    "# set max rows for pandas dataframe to 100\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "DATA_PATH = \"data/play_by_play/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>time_remaining</th>\n",
       "      <th>home_team_win_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>3600</td>\n",
       "      <td>0.5296259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>3547</td>\n",
       "      <td>0.5296259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>3535</td>\n",
       "      <td>0.5296259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>3520</td>\n",
       "      <td>0.5296259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.5296259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10625</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>-64</td>\n",
       "      <td>0.4782254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10626</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>-85</td>\n",
       "      <td>0.4782254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10627</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>-99</td>\n",
       "      <td>0.4782254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10628</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>-116</td>\n",
       "      <td>0.4782254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10629</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>-136</td>\n",
       "      <td>0.7602914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10630 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          game_id  time_remaining  home_team_win_probability\n",
       "0      2022020839            3600                  0.5296259\n",
       "1      2022020839            3547                  0.5296259\n",
       "2      2022020839            3535                  0.5296259\n",
       "3      2022020839            3520                  0.5296259\n",
       "4      2022020839            3500                  0.5296259\n",
       "...           ...             ...                        ...\n",
       "10625  2023020482             -64                  0.4782254\n",
       "10626  2023020482             -85                  0.4782254\n",
       "10627  2023020482             -99                  0.4782254\n",
       "10628  2023020482            -116                  0.4782254\n",
       "10629  2023020482            -136                  0.7602914\n",
       "\n",
       "[10630 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>game_seconds</th>\n",
       "      <th>probability_home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6458669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>49</td>\n",
       "      <td>0.6176999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>87</td>\n",
       "      <td>0.6218961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>93</td>\n",
       "      <td>0.4791775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022020839</td>\n",
       "      <td>102</td>\n",
       "      <td>0.4568295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600364</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>3486</td>\n",
       "      <td>0.9936139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600365</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>3524</td>\n",
       "      <td>0.9936139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600366</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>3528</td>\n",
       "      <td>0.9936544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600367</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>3531</td>\n",
       "      <td>0.9936544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600368</th>\n",
       "      <td>2023020482</td>\n",
       "      <td>3543</td>\n",
       "      <td>0.9936544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2600369 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            game_id  game_seconds  probability_home\n",
       "0        2022020839             0         0.6458669\n",
       "1        2022020839            49         0.6176999\n",
       "2        2022020839            87         0.6218961\n",
       "3        2022020839            93         0.4791775\n",
       "4        2022020839           102         0.4568295\n",
       "...             ...           ...               ...\n",
       "2600364  2023020482          3486         0.9936139\n",
       "2600365  2023020482          3524         0.9936139\n",
       "2600366  2023020482          3528         0.9936544\n",
       "2600367  2023020482          3531         0.9936544\n",
       "2600368  2023020482          3543         0.9936544\n",
       "\n",
       "[2600369 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game_ids = [2022020839, 2023020412, 2022020727, 2022030154, \n",
    "    2022021296, 2023020164, 2022020793, 2023020317, \n",
    "    2022020971, 2022030145, 2022020938, 2022021300, \n",
    "    2022021101, 2022020747, 2023020254, 2022020832, \n",
    "    2022020842, 2022021012, 2023020268, 2022020867, \n",
    "    2022030324, 2023020546, 2022021001, 2022021055, \n",
    "    2023020464, 2022021181, 2023020096, 2022020688, \n",
    "    2023020325, 2022021166, 2022020932, 2022020843, \n",
    "    2023020365, 2022030245, 2022021040, 2022021088, \n",
    "    2023020570, 2023020264, 2023020129, 2022020876, \n",
    "    2023020292, 2023020010, 2022021270, 2022021115, \n",
    "    2022020753, 2023020475, 2023020192, 2022020662, \n",
    "    2023020033, 2023020482]\n",
    "\n",
    "for game_id in game_ids:\n",
    "    path = f\"data/pickles/simulation_probs_{game_id}.pickle\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "\n",
    "bayesian_df = pd.read_feather(\"data/predictions/bayesian_outcomes.feather\")\n",
    "xgboost_df = pd.read_feather(\"data/predictions/xgboost_results_df.feather\")\n",
    "display(bayesian_df)\n",
    "display(xgboost_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulate a game and save probabilities along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_files.simulator import get_simulation_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_feather(\"data/play_by_play/play_by_play_full_state_space.feather\")\n",
    "display(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the probability table\n",
    "prob_mc = pd.read_csv(\"data/probability_tables/probabilities_avg_NOHIT.csv\")\n",
    "prob_mc = prob_mc[['prev3', 'prev2', 'prev1', 'curr_event', 'probability_avg']]\n",
    "prob_mc = prob_mc.rename(columns={'probability_avg': 'probability'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_prob_mc = prob_mc.copy()\n",
    "prob_sums = norm_prob_mc.groupby(['prev3', 'prev2', 'prev1'])['probability'].sum()\n",
    "norm_prob_mc = pd.merge(norm_prob_mc, prob_sums, on=['prev3', 'prev2', 'prev1'], suffixes=('', '_sum'))\n",
    "norm_prob_mc['probability'] = norm_prob_mc['probability'] / norm_prob_mc['probability_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full.copy()\n",
    "\n",
    "random.seed(42)\n",
    "game_id = choice(df['game_id'].unique())\n",
    "\n",
    "curr_game = df.loc[df['game_id'] == game_id, :].copy()\n",
    "home_name = curr_game['home_name'].values[0]\n",
    "away_name = curr_game['away_name'].values[0]\n",
    "game_date = curr_game['game_date'].values[0]\n",
    "\n",
    "simulation_cols = ['game_seconds_remaining'] + [c for c in curr_game.columns if 'STATE' in c]\n",
    "simulation_cols_new = [c.replace(\"STATE_\", \"\") for c in simulation_cols]\n",
    "state_portion = (curr_game\n",
    "    .loc[:, simulation_cols]\n",
    "    .copy()\n",
    "    .reset_index(drop=True)\n",
    "    .rename(columns=dict(zip(simulation_cols, simulation_cols_new))))\n",
    "\n",
    "# load the probability table\n",
    "prob_mc = pd.read_csv(\"data/probability_tables/probabilities_avg_NOHIT.csv\")\n",
    "prob_mc = prob_mc[['prev3', 'prev2', 'prev1', 'curr_event', 'probability_avg']]\n",
    "prob_mc = prob_mc.rename(columns={'probability_avg': 'probability'})\n",
    "\n",
    "with open(\"data/pickles/kde_seconds_NOHIT.pickle\", \"rb\") as f:\n",
    "    kde_seconds = pickle.load(f)\n",
    "    \n",
    "# sample from the game every minute\n",
    "rows_to_simulate = list(range(0, len(state_portion), len(state_portion) // 10))\n",
    "simulation_probabilities = []\n",
    "for ind in rows_to_simulate:\n",
    "    inds = np.arange(max(0, ind-4), ind+1)\n",
    "    curr_snapshot = state_portion.loc[inds, :].copy()\n",
    "    print(\"Seconds remaining:\", curr_snapshot['game_seconds_remaining'].values[-1], \" \"*30, end=\"\\r\")\n",
    "    \n",
    "    home_prob, away_prob = get_simulation_prob(curr_snapshot, prob_mc, kde_seconds, verbose=True)\n",
    "    simulation_probabilities.append((home_prob, away_prob))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simulation_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(curr_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = curr_game.copy()\n",
    "thing['STATE_GOAL_HOME_diff'] = thing['STATE_GOAL_HOME'].diff().fillna(0)\n",
    "goals_rows = thing['STATE_GOAL_HOME_diff'] != 0\n",
    "home_goals = thing.loc[goals_rows, 'game_seconds'].values\n",
    "\n",
    "thing['STATE_GOAL_AWAY_diff'] = thing['STATE_GOAL_AWAY'].diff().fillna(0)\n",
    "goals_rows = thing['STATE_GOAL_AWAY_diff'] != 0\n",
    "away_goals = thing.loc[goals_rows, 'game_seconds'].values\n",
    "\n",
    "inds = thing.loc[goals_rows, :].index\n",
    "inds = np.concatenate([inds-1, inds, inds+1])\n",
    "inds = np.unique(inds)\n",
    "inds = np.sort(inds)\n",
    "thing = movecol(thing, ['STATE_GOAL_HOME_diff'], 'STATE_GOAL_HOME', 'After')\n",
    "display(thing.loc[inds, ['STATE_GOAL_HOME', 'STATE_GOAL_HOME_diff']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in team colors\n",
    "with open('team_colors.json') as f:\n",
    "    team_colors = json.load(f)\n",
    "\n",
    "# Get the game data\n",
    "game_data = curr_game.copy()\n",
    "team, opp = game_data['home_name'].values[0], game_data['away_name'].values[0]\n",
    "\n",
    "# Get who the home team is\n",
    "home_team = team\n",
    "away_team = opp\n",
    "\n",
    "# Get colors for home and away team\n",
    "home_color = team_colors[home_team]\n",
    "away_color = team_colors[away_team]\n",
    "\n",
    "# Predict the win probability\n",
    "y_pred = deepcopy(simulation_probabilities)\n",
    "\n",
    "time_vals = curr_game['game_seconds'].values[rows_to_simulate]\n",
    "pred_vals = np.array([p[0] for p in y_pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# Plot the probabilities\n",
    "plt.figure(figsize=(10, 5), dpi=200)\n",
    "plt.plot(time_vals, pred_vals, color='black')\n",
    "\n",
    "home_where = pred_vals >= threshold if home_team == team else pred_vals < threshold\n",
    "away_where = pred_vals < threshold if home_team == team else pred_vals >= threshold\n",
    "\n",
    "# Fill area above 50% with blue for Home Team\n",
    "plt.fill_between(time_vals, threshold, pred_vals, where=pred_vals >= threshold, color=home_color['main'], interpolate=True, alpha=0.5)\n",
    "\n",
    "# Fill area below 50% with red for Home Team\n",
    "plt.fill_between(time_vals, threshold, pred_vals, where=pred_vals < threshold, color=away_color['main'], interpolate=True, alpha=0.5)\n",
    "\n",
    "# Add a dotted horizontal line at y = 0.5\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--')\n",
    "plt.yticks(ticks=[0.0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0], labels=['100%', '90%', '80%', '70%', '60%', '50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "\n",
    "# Plot when the goal was scored\n",
    "plt.vlines(home_goals, 0, 1, color='green', linestyle='--', label=f'{home_team} Goal')\n",
    "plt.vlines(away_goals, 0, 1, color='purple', linestyle='--', label=f'{away_team} Goal')\n",
    "\n",
    "# Plot periods\n",
    "# plt.vlines([1200, 2400, 3600], 0, 1, color='black', linestyle='-', label='Period End')\n",
    "plt.xticks(ticks=[0, 1200, 2400, 3600], labels=['Game Start', 'End 1st', 'End 2nd', 'End 3rd'])\n",
    "plt.title(f'{away_team} ({len(away_goals)}) @ {home_team} ({len(home_goals)})')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_and_save_chart(game_id):\n",
    "    \n",
    "    path = f\"data/pickles/simulation_probs_{game_id}.pickle\"\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError(f\"{game_id} does not have simulated results in data/pickles\")\n",
    "    \n",
    "    with open(path, 'rb') as file:\n",
    "        simulation_res = pickle.load(file)\n",
    "    game_id = simulation_res['game_id']\n",
    "    curr_game = simulation_res['curr_game']\n",
    "    rows_to_simulate = simulation_res['rows_to_simulate']\n",
    "    simulation_probabilities = simulation_res['simulation_probabilities']\n",
    "    \n",
    "    # make the Montreal Candiens name uniform without special characters\n",
    "    mc1 = \"Montréal Canadiens\"\n",
    "    mc2 = \"MontrÃ©al Canadiens\"\n",
    "    mc_actual = \"Montreal Canadiens\"\n",
    "    curr_game.loc[curr_game['home_name'] == mc1, 'home_name'] = mc_actual\n",
    "    curr_game.loc[curr_game['away_name'] == mc1, 'away_name'] = mc_actual\n",
    "    curr_game.loc[curr_game['home_name'] == mc2, 'home_name'] = mc_actual\n",
    "    curr_game.loc[curr_game['away_name'] == mc2, 'away_name'] = mc_actual\n",
    "    \n",
    "    # get the times of the home and away goals\n",
    "    thing = curr_game.copy()\n",
    "    thing['STATE_GOAL_HOME_diff'] = thing['STATE_GOAL_HOME'].diff().fillna(0)\n",
    "    goals_rows = thing['STATE_GOAL_HOME_diff'] != 0\n",
    "    home_goals = thing.loc[goals_rows, 'game_seconds'].values\n",
    "\n",
    "    thing['STATE_GOAL_AWAY_diff'] = thing['STATE_GOAL_AWAY'].diff().fillna(0)\n",
    "    goals_rows = thing['STATE_GOAL_AWAY_diff'] != 0\n",
    "    away_goals = thing.loc[goals_rows, 'game_seconds'].values\n",
    "    \n",
    "    # Read in team colors\n",
    "    with open('team_colors.json') as f:\n",
    "        team_colors = json.load(f)\n",
    "\n",
    "    # Get the game data\n",
    "    game_data = curr_game.copy()\n",
    "    team, opp = game_data['home_name'].values[0], game_data['away_name'].values[0]\n",
    "\n",
    "    # Get who the home team is\n",
    "    home_team = team\n",
    "    away_team = opp\n",
    "\n",
    "    # Get colors for home and away team\n",
    "    home_color = team_colors[home_team]\n",
    "    away_color = team_colors[away_team]\n",
    "\n",
    "    # Predict the win probability\n",
    "    y_pred = deepcopy(simulation_probabilities)\n",
    "\n",
    "    time_vals = curr_game['game_seconds'].values[rows_to_simulate]\n",
    "    pred_vals = np.array([p[0] for p in y_pred])\n",
    "    \n",
    "    # Set threshold\n",
    "    threshold = 0.5\n",
    "\n",
    "    # Plot the probabilities\n",
    "    plt.figure(figsize=(10, 5), dpi=200)\n",
    "    plt.plot(time_vals, pred_vals, color='black')\n",
    "\n",
    "    home_where = pred_vals >= threshold if home_team == team else pred_vals < threshold\n",
    "    away_where = pred_vals < threshold if home_team == team else pred_vals >= threshold\n",
    "\n",
    "    # Fill area above 50% with blue for Home Team\n",
    "    plt.fill_between(time_vals, threshold, pred_vals, where=pred_vals >= threshold, color=home_color['main'], interpolate=True, alpha=0.5)\n",
    "\n",
    "    # Fill area below 50% with red for Home Team\n",
    "    plt.fill_between(time_vals, threshold, pred_vals, where=pred_vals < threshold, color=away_color['main'], interpolate=True, alpha=0.5)\n",
    "\n",
    "    # Add a dotted horizontal line at y = 0.5\n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--')\n",
    "    plt.yticks(ticks=[0.0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0], labels=['100%', '90%', '80%', '70%', '60%', '50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "\n",
    "    # Plot when the goal was scored\n",
    "    plt.vlines(home_goals, 0, 1, color='green', linestyle='--', label=f'{home_team} Goal')\n",
    "    plt.vlines(away_goals, 0, 1, color='purple', linestyle='--', label=f'{away_team} Goal')\n",
    "\n",
    "    # Plot periods\n",
    "    # plt.vlines([1200, 2400, 3600], 0, 1, color='black', linestyle='-', label='Period End')\n",
    "    plt.xticks(ticks=[0, 1200, 2400, 3600], labels=['Game Start', 'End 1st', 'End 2nd', 'End 3rd'])\n",
    "    plt.title(f'{away_team} ({len(away_goals)}) @ {home_team} ({len(home_goals)})')\n",
    "    plt.ylabel(\"Away Team        |        Home Team\")\n",
    "    plt.legend()\n",
    "    \n",
    "    a, ag, h, hg = away_team, len(away_goals), home_team, len(home_goals)\n",
    "    plt.savefig(f\"Latex/images/mcmc_{a}_{ag}_{h}_{hg}_{game_id}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(f\"Latex/images/mcmc_{a}_{ag}_{h}_{hg}_{game_id}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "# loop over every file name in data/pickles\n",
    "directory = \"data/pickles\"\n",
    "for file_name in os.listdir(directory):\n",
    "    path = os.path.join(directory, file_name)\n",
    "    if os.path.exists(path) and \"simulation_probs\" in file_name:\n",
    "        game_id = int(file_name.split(\"_\")[-1].split(\".\")[0])\n",
    "        display_and_save_chart(game_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text by converting special characters to their normal counterparts.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Input text with special characters.\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with special characters converted to their normal counterparts.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "# Example usage:\n",
    "text_with_special_characters = \"MontrÃ©al Canadiens\"\n",
    "normalized_text = normalize_text(text_with_special_characters)\n",
    "print(normalized_text)  # Output: Montreal Canadiens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check kde ~ poisson(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pickles/kde_seconds_NOHIT.pickle\", \"rb\") as f:\n",
    "    kde_seconds = pickle.load(f)\n",
    "    \n",
    "sample = kde_seconds.sample(100000)\n",
    "\n",
    "poisson_sample = np.random.poisson(18, 100000)\n",
    "\n",
    "plt.hist(sample, bins=100, alpha=0.5, label=\"kde\", density=True)\n",
    "plt.hist(poisson_sample, bins=100, alpha=0.5, label=\"poisson\", density=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code for the writeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pd.read_csv(\"data/probability_tables/probabilities_avg_NOHIT.csv\")\n",
    "display(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs['curr_event'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pickles/kde_seconds_NOHIT.pickle\", \"rb\") as f:\n",
    "    kde_seconds = pickle.load(f)\n",
    "    \n",
    "sample = kde_seconds.sample(10000000)\n",
    "print(np.mean(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_events = \"SHOT_AWAY,BLOCKED_SHOT_HOME,TAKEAWAY_HOME\"\n",
    "prev3 = \"FACEOFF_HOME\"\n",
    "prev2 = \"GIVEAWAY_HOME\"\n",
    "prev1 = \"MISSED_SHOT_AWAY\"\n",
    "curr_probs = (probs\n",
    "    .loc[\n",
    "        (probs['prev3'] == prev3) & \n",
    "        (probs['prev2'] == prev2) & \n",
    "        (probs['prev1'] == prev1) &\n",
    "        (probs['curr_event'] != 'BLOCKED_SHOT_-'), \n",
    "        ['prev3', 'prev2', 'prev1', 'curr_event', 'probability_avg']]\n",
    "    .copy()\n",
    "    .sort_values(by='probability_avg', ascending=False)\n",
    "    .rename(columns={'probability_avg': 'probability', 'curr_event':'next_event'})\n",
    ")\n",
    "#curr_probs = curr_probs.loc[(curr_probs['next_event'] != 'BLOCKED_SHOT_-'), :]\n",
    "curr_probs['curr_state'] = curr_probs['prev3'].astype(str) + \",\" + curr_probs['prev2'].astype(str) + \",\" + curr_probs['prev1'].astype(str)\n",
    "curr_probs = curr_probs[['curr_state', 'next_event', 'probability']]\n",
    "display(curr_probs.head(4))\n",
    "print_display('curr_probs', globals(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['away_strength_code'] = 'EV'\n",
    "df.loc[df['strength_code'] == 'PP', 'away_strength_code'] = 'SH'\n",
    "df.loc[df['strength_code'] == 'SH', 'away_strength_code'] = 'PP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load full state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_files.data_manager import create_state_space_opt\n",
    "df = create_state_space_opt('data/play_by_play/play_by_play_full.feather', True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True).to_feather('data/play_by_play/play_by_play_full_state_space.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze effectiveness of our simulator vs. actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full state space feather\n",
    "df = pd.read_feather('data/play_by_play/play_by_play_full_state_space.feather')\n",
    "\n",
    "simulation_500 = pd.read_feather('data/simulated_games/nohit_500.feather')\n",
    "simulation_1000 = pd.read_feather('data/simulated_games/nohit_1000.feather')\n",
    "\n",
    "simulation_500['game_id'] = (simulation_500['game_id'].astype(int) + 2000).astype(str).str.zfill(5)\n",
    "# load the simulated games\n",
    "simulations = pd.concat([simulation_1000, simulation_500])\n",
    "display(df)\n",
    "display(simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies of the dataframes to make this cell re-runnable\n",
    "simulated = simulations.copy()\n",
    "actual = df.copy()\n",
    "\n",
    "# only keep the relvenat state columns\n",
    "state_cols = [c for c in actual.columns if 'STATE' in c]\n",
    "rel_cols = ['game_id'] + state_cols\n",
    "actual = actual[rel_cols]\n",
    "\n",
    "simulated_state_cols = [c.replace(\"STATE_\", \"\") for c in state_cols]\n",
    "actual = actual.rename(columns={c: c.replace(\"STATE_\", \"\") for c in state_cols})\n",
    "rel_cols = ['game_id'] + simulated_state_cols\n",
    "simulated = simulated[rel_cols]\n",
    "\n",
    "# only keep the last rows\n",
    "sim_last_rows = simulated['game_id'] != simulated['game_id'].shift(-1)\n",
    "simulated = simulated.loc[sim_last_rows, :]\n",
    "actual_last_rows = actual['game_id'] != actual['game_id'].shift(-1)\n",
    "actual = actual.loc[actual_last_rows, :]\n",
    "\n",
    "actual = actual.sample(n=1500)\n",
    "\n",
    "print(\"Simulated last rows\")\n",
    "display(simulated)\n",
    "print(\"actual last rows\")\n",
    "display(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cols = [c for c in actual.columns if c != 'game_id']\n",
    "print(len(event_cols))\n",
    "\n",
    "for i, col in enumerate(event_cols):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    actual_vals = actual.sample(n=1500)[col].values\n",
    "    simulated_vals = simulated[col].values\n",
    "    \n",
    "    actual_vals_probs = np.histogram(actual_vals, bins=50, density=True)[0]\n",
    "    simulated_vals_probs = np.histogram(simulated_vals, bins=50, density=True)[0]\n",
    "    # compute the KL divergence between actual_vals and simulated_vals\n",
    "    emd = wasserstein_distance(actual_vals_probs, simulated_vals_probs)\n",
    "    plt.hist(actual_vals, bins=50, color='green', alpha=0.5, label='Actual', edgecolor='white', density=True)\n",
    "    plt.hist(simulated_vals, bins=50, color='red', alpha=0.5, label='Simulated', edgecolor='white', density=True)\n",
    "    plt.legend()\n",
    "    plt.title(f\"{col}, EMD: {emd:.2f}\")\n",
    "\n",
    "plt.gcf().set_size_inches(20, 15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels\n",
    "actual[\"simulated\"] = 0\n",
    "simulated[\"simulated\"] = 1\n",
    "\n",
    "# combine the actual and simulated games and shuffle the rows\n",
    "vals = (\n",
    "    pd.concat([\n",
    "           actual.drop(columns='game_id').copy(), # only use 1700 of the actual games\n",
    "           simulated.drop(columns='game_id').copy()])\n",
    "    .sample(frac=1.0) # shuffle the rows\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "X, y = vals.drop(columns=[\"simulated\"]), vals.simulated\n",
    "X.fillna(0, inplace=True)\n",
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into train and test with 70-30\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# run a kmeans with two clusters on X and y\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X_train, y_train)\n",
    "y_pred = kmeans.predict(X_test)\n",
    "print(\"Accuracy of KMeans:\", round(np.sum(y_pred == y_test) / len(y_test), 4))\n",
    "\n",
    "# run another classifier on X and y and print the accuracy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Accuracy of Logistic Regression:\", round(np.sum(y_pred == y_test) / len(y_test), 4))\n",
    "\n",
    "# run an XGBoost classifier on X and y and print the accuracy\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(random_state=0)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(\"Accuracy of XGBoost:\", round(np.sum(y_pred == y_test) / len(y_test), 4))\n",
    "\n",
    "# run a random forest classifier on X and y and print the accuracy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy of Random Forest:\", round(np.sum(y_pred == y_test) / len(y_test), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels\n",
    "actual[\"simulated\"] = 0\n",
    "simulated[\"simulated\"] = 1\n",
    "\n",
    "# combine the actual and simulated games and shuffle the rows\n",
    "vals = (\n",
    "    pd.concat([\n",
    "           actual.copy().sample(frac=0.1), # only use 1700 of the actual games\n",
    "           simulated.copy()])\n",
    "    .sample(frac=1.0) # shuffle the rows\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "X, y = vals.drop(columns=[\"simulated\"]), vals.simulated\n",
    "\n",
    "# Fill all NaN's with 0\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "vals = {}\n",
    "\n",
    "pbar = tqdm(total=9)\n",
    "# Perform PCA, t-SNE, and UMAP on the data\n",
    "pbar.set_description(\"Performing PCA\")\n",
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "pbar.update(1)\n",
    "\n",
    "vals[(0, 0)] = X_pca\n",
    "\n",
    "for perp, neigh in [(50, 25), (100, 50), (250, 150), (500, 300)]:\n",
    "    # Compute t-SNE\n",
    "    pbar.set_description(f\"Performing t-SNE (Perplexity = {perp})\")\n",
    "    X_tsne = TSNE(n_components=2, perplexity=perp).fit_transform(X)\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Compute UMAP\n",
    "    pbar.set_description(f\"Performing UMAP (Neighbors = {neigh})\")\n",
    "    X_umap = UMAP(n_components=2, n_neighbors=neigh).fit_transform(X)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    vals[(perp, neigh)] = (X_tsne, X_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for key, value in vals.items():\n",
    "    if key == (0,0):\n",
    "        continue\n",
    "    perp, neigh = key\n",
    "    X_tsne, X_umap = value\n",
    "    \n",
    "    plt.subplot()\n",
    "    i += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Do t-SNE, UMAP, and PCA on the simulated data against the real data\n",
    "# Simulated data\n",
    "simulated = pd.read_feather('data/dylan_data/lasts.feather')\n",
    "actual = last_row.copy()\n",
    "actual.drop(columns=['TIME_REMAINING', 'WIN', 'GAME_ID', 'AWAY_CORSI_FOR',\n",
    "       'AWAY_FENWICK_FOR', 'HOME_CORSI_FOR', 'HOME_FENWICK_FOR', 'AWAY_CORSI', 'AWAY_FENWICK',\n",
    "       'AWAY_GAME_ID', 'AWAY_TIME_REMAINING', 'AWAY_WIN', 'HOME_CORSI',\n",
    "       'HOME_FENWICK', 'HOME_GAME_ID', 'HOME_HIT', 'AWAY_HIT',\n",
    "       'HOME_TEAM', 'HOME_TIME_REMAINING', 'HOME_WIN', 'AWAY_HOME','AWAY_TEAM', 'HOME_HOME',\n",
    "       ], inplace=True)\n",
    "\n",
    "simulated.drop(columns=['time_remaining', 'home_score', 'away_score', 'game_id', 'BLOCKED_SHOT_-'], inplace=True)\n",
    "actual = actual[actual.columns.sort_values()]\n",
    "simulated = simulated[simulated.columns.sort_values()]\n",
    "simulated.columns = [f'{col[-1]}_{\"_\".join(col[:-1])}' for col in simulated.columns.str.split('_')]\n",
    "simulated = simulated[simulated.columns.sort_values()]\n",
    "\n",
    "# Assign labels\n",
    "actual['simulated'] = 0\n",
    "simulated['simulated'] = 1\n",
    "\n",
    "vals = actual.append(simulated)\n",
    "X, y = vals.drop(columns=['simulated']), vals.simulated\n",
    "\n",
    "# Fill all NaN's with 0\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Perform PCA, t-SNE, and UMAP on the data\n",
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize=(16, 10))\n",
    "i = 0\n",
    "for perp, neigh in [[100, 50], [250, 150], [500, 300]]:\n",
    "       # Compute t-SNE\n",
    "       X_tsne = TSNE(n_components=2, perplexity=perp).fit_transform(X)\n",
    "       \n",
    "       # Compute UMAP\n",
    "       X_umap = UMAP(n_components=2, n_neighbors=neigh).fit_transform(X)\n",
    "       \n",
    "       # Plot the results\n",
    "       ax[i, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "       ax[i, 0].set_title('PCA')\n",
    "       \n",
    "       ax[i, 1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "       ax[i, 1].set_title(f't-SNE (Perplexity = {perp})')\n",
    "       \n",
    "       ax[i, 2].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "       ax[i, 2].set_title(f'UMAP (Neighbors = {neigh}')\n",
    "       \n",
    "       i += 1\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load ALL of the data and clean it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for file in sorted(list(os.listdir(DATA_PATH))):\n",
    "    if file.endswith(\".csv\") and 'play_by_play' in file:\n",
    "        print(file)\n",
    "        df = load_and_clean_csv(os.path.join(DATA_PATH, file))\n",
    "        df.reset_index(drop=True).to_feather(os.path.join(DATA_PATH, file.replace(\".csv\", \"_clean.feather\")))\n",
    "        dataframes.append(df)\n",
    "\n",
    "df_orig = pd.concat(dataframes, ignore_index=True)\n",
    "df_orig = df_orig.sort_values(by=[\"date_time\", \"game_id\", \"game_seconds\"]).reset_index(drop=True)\n",
    "df_orig.reset_index(drop=True).to_feather(os.path.join(DATA_PATH, \"play_by_play_full.feather\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the full play_by_play feather fule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_feather(os.path.join(DATA_PATH, \"play_by_play_full.feather\"))\n",
    "display(df_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the net_player feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_net = df_orig.copy()\n",
    "df_net['home_skaters'] = df_net['home_skaters'].astype(int)\n",
    "df_net['away_skaters'] = df_net['away_skaters'].astype(int)\n",
    "\n",
    "# forward fill the home_skaters zero values\n",
    "df_net['home_skaters'] = df_net['home_skaters'].replace(0, np.nan).ffill().astype(int)\n",
    "df_net['away_skaters'] = df_net['away_skaters'].replace(0, np.nan).ffill().astype(int)\n",
    "\n",
    "# compute a new strength_state column that is always in perspective of home\n",
    "df_net['strength_state'] = df_net['home_skaters'].copy().astype(str) + 'v' + df_net['away_skaters'].copy().astype(str)\n",
    "\n",
    "# create the net_players column that keeps track of when home or away are in a power play or short handed\n",
    "df_net['net_players'] = 'EV'\n",
    "df_net.loc[df_net['home_skaters'] > df_net['away_skaters'], 'net_players'] = 'PP'\n",
    "df_net.loc[df_net['home_skaters'] < df_net['away_skaters'], 'net_players'] = 'SH'\n",
    "display(df_net['net_players'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup df_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_net.copy()\n",
    "df = pd.read_feather('data/play_by_play/play_by_play_full_state_space.feather')\n",
    "\n",
    "# consolidate the overall dataframe into a dataframe with just events and the net_players entry\n",
    "#df_events = df[['game_id', 'event_type', 'game_seconds', 'event_team_type', 'net_players']].copy()\n",
    "df_events = df[['game_id', 'event_type', 'game_seconds', 'event_team_type']].copy()\n",
    "\n",
    "# ignore the HIT events\n",
    "df_events = df_events.loc[~(df_events['event_type'] == 'HIT'), :].copy()\n",
    "\n",
    "'''df_events['event'] = (\n",
    "    df_events['event_type'].astype(str) \n",
    "    +'_' + df_events['event_team_type'].astype(str).str.upper()  \n",
    "    #+ \"_\" + df_events['net_players'].astype(str)\n",
    ")'''\n",
    "df_events['event'] = df_events['event_type'].astype(str)\n",
    "df_events = df_events.sort_values(by=['game_id', 'game_seconds'], ascending=[True, True])\n",
    "df_events = df_events[['game_id', 'event', 'game_seconds']]\n",
    "\n",
    "first_rows = df_events['game_id'] != df_events['game_id'].shift(1)\n",
    "df_events['seconds_diff'] = np.abs(df_events['game_seconds'].diff().fillna(0))\n",
    "df_events.loc[first_rows, 'seconds_diff'] = 0\n",
    "df_events = df_events.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute the probability tables for the MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_probabilities(probabilities:pd.DataFrame, probability_col_name:str):\n",
    "    \"\"\" takes in a dataframe with columns 'prev_events' and probability_col_name\n",
    "    and normalizes the probability_col_name column for all probabilities in each\n",
    "    prev_events group\n",
    "    \"\"\"\n",
    "    col_name_sum = probability_col_name + '_sum'\n",
    "    \n",
    "    probs_sums = (probabilities\n",
    "        .copy()\n",
    "        .groupby(by=['prev_events'])[probability_col_name]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={probability_col_name: col_name_sum}))\n",
    "    \n",
    "    probabilities = pd.merge(probabilities, probs_sums, on=['prev_events'], how='left')\n",
    "    probabilities[probability_col_name] /= probabilities[col_name_sum]\n",
    "    probabilities = probabilities.drop(columns=[col_name_sum])\n",
    "    probabilities = probabilities.fillna(0)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def compute_probabilities(df, prev_count=3):\n",
    "    \"\"\" takes in a dataframe with the columns \"game_id\", \"event_type\", \"game_seconds\", and \"event_team_type\"\n",
    "    and an integer prev_count that represents the number of previous events to consider.\n",
    "    Returns a dataframe with the columns \"prev_events\", \"curr_event\", \"probability\", \"given_count\", \"join_count\"\n",
    "    where prev_events will have 'prev_count' event names separated by commas\n",
    "    \n",
    "    probabilities df\n",
    "    prev_events\t                     curr_event\t    probability\tgiven_count\tjoin_count\n",
    "    BLOCKED_SHOT_AWAY,GOAL_AWAY,BLOCKED_SHOT_AWAY\tFACEOFF_HOME\t0.86\t86\t100\n",
    "    MISSED_SHOT_HOME,PENALTY_HOME,BLOCKED_SHOT_HOME\tFACEOFF_AWAY\t0.84\t84\t100\n",
    "    \"\"\"\n",
    "    df_cols = ['game_id', 'event_type', 'game_seconds', 'event_team_type']\n",
    "    if not all([col in df.columns for col in df_cols]):\n",
    "        raise ValueError(f\"Dataframe must have the columns {df_cols}\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    ############################################################\n",
    "    #                     CREATE DF_EVENTS                     #\n",
    "    ############################################################\n",
    "\n",
    "    # consolidate the overall dataframe into a dataframe with just events and the net_players entry\n",
    "    #df_events = df[['game_id', 'event_type', 'game_seconds', 'event_team_type', 'net_players']].copy()\n",
    "    df_events = df[['game_id', 'event_type', 'game_seconds', 'event_team_type']].copy()\n",
    "\n",
    "    # ignore the HIT events\n",
    "    df_events = df_events.loc[~(df_events['event_type'] == 'HIT'), :].copy()\n",
    "\n",
    "    df_events['event'] = (\n",
    "        df_events['event_type'].astype(str) \n",
    "        +'_' + df_events['event_team_type'].astype(str).str.upper()  \n",
    "        #+ \"_\" + df_events['net_players'].astype(str)\n",
    "    )\n",
    "    \n",
    "    replaces = [('_HOME_HOME', '_HOME'), ('_AWAY_AWAY', '_AWAY')]\n",
    "    for orig, new in replaces:\n",
    "        df_events['event'] = df_events['event'].str.replace(orig, new)\n",
    "    \n",
    "    df_events = df_events.sort_values(by=['game_id', 'game_seconds'], ascending=[True, True])\n",
    "    df_events = df_events[['game_id', 'event', 'game_seconds']]\n",
    "\n",
    "    first_rows = df_events['game_id'] != df_events['game_id'].shift(1)\n",
    "    df_events['seconds_diff'] = np.abs(df_events['game_seconds'].diff().fillna(0))\n",
    "    df_events.loc[first_rows, 'seconds_diff'] = 0\n",
    "    df_events = df_events.reset_index(drop=True)\n",
    "    \n",
    "    ############################################################\n",
    "    #               CREATE THE MARKOV CHAIN TABLE              #\n",
    "    ############################################################\n",
    "    \n",
    "    prev_cols = [f'prev{i}' for i in range(1, prev_count+1)]\n",
    "\n",
    "    # only keep the event and the game_id\n",
    "    df_mc = df_events[['game_id', 'event']].copy()\n",
    "    \n",
    "    row_masks = []\n",
    "    for prev_i in range(1, prev_count+1):\n",
    "        \n",
    "        # find the current ith rows for each game_id\n",
    "        curr_rows = df_mc['game_id'] != df_mc['game_id'].shift(prev_i)\n",
    "        \n",
    "        # create the current mask and the join mask\n",
    "        curr_mask = curr_rows.copy()\n",
    "        join_mask = curr_mask.copy()\n",
    "        for row_mask in row_masks:\n",
    "            curr_mask &= (~row_mask)\n",
    "            join_mask |= row_mask\n",
    "            \n",
    "        # add the prev<i> column and make sure that the shift doesn't carry over to the next game\n",
    "        df_mc[f'prev{prev_i}'] = df_mc['event'].shift(prev_i)\n",
    "        df_mc.loc[join_mask, f'prev{prev_i}'] = \"#\"\n",
    "        \n",
    "        row_masks.append(curr_mask)\n",
    "        \n",
    "    # create the prev_events column as a series with empty strings\n",
    "    curr_prev_events = pd.Series([\"\" for _ in range(df_mc.shape[0])], index=df_mc.index.copy())\n",
    "    \n",
    "    for prev_i in range(prev_count, 0, -1):\n",
    "        if prev_i != 1:\n",
    "            curr_prev_events += df_mc[f'prev{prev_i}'].astype(str) + \",\"\n",
    "        else:\n",
    "            curr_prev_events += df_mc[f'prev{prev_i}'].astype(str)\n",
    "    df_mc['prev_events'] = curr_prev_events\n",
    "            \n",
    "    df_mc = df_mc.dropna()\n",
    "    df_mc['curr_event'] = df_mc['event']\n",
    "    \n",
    "    with open(f\"data/pickles/temp_{prev_count}.pickle\", \"wb\") as f:\n",
    "        pickle.dump(df_mc, f)\n",
    "    \n",
    "    ############################################################\n",
    "    #  COMPUTE THE PROBABILITIES AS P(B|A) = P(A AND B) / P(A) #\n",
    "    ############################################################\n",
    "    \n",
    "    # create a probabilities dataframe that continas the probability of moving from\n",
    "    # prev3-prev2-prev1 -> curr_event\n",
    "    prev_events = df_mc['prev_events'].unique()\n",
    "    curr_events = df_mc['curr_event'].unique()\n",
    "\n",
    "    probabilities = pd.DataFrame(list(product(prev_events, curr_events)), columns=['prev_events', 'curr_event'])\n",
    "    probabilities['join'] = probabilities['prev_events'] + '|' + probabilities['curr_event']\n",
    "\n",
    "    # compute the given-this counts\n",
    "    given_counts = df_mc['prev_events'].value_counts()\n",
    "    given_counts = pd.DataFrame(given_counts).reset_index()\n",
    "    given_counts.columns = ['prev_events', 'given_count']\n",
    "\n",
    "    # compute the joint counts\n",
    "    df_mc['join'] = df_mc['prev_events'] + '|' + df_mc['curr_event']\n",
    "    join_counts = df_mc['join'].value_counts()\n",
    "    join_counts = pd.DataFrame(join_counts).reset_index()\n",
    "    join_counts.columns = ['join', 'join_count']\n",
    "\n",
    "    # join the counts onto the probabilities dataframe\n",
    "    probabilities = probabilities.merge(given_counts, on='prev_events', how='left')\n",
    "    probabilities = probabilities.merge(join_counts, on='join', how='left')\n",
    "    probabilities = probabilities.fillna(0)\n",
    "    probabilities['probability'] = probabilities['join_count'] / probabilities['given_count']\n",
    "    probabilities = probabilities.sort_values(by='probability', ascending=False)\n",
    "    probabilities = probabilities[['prev_events', 'curr_event', 'probability', 'given_count', 'join_count']]\n",
    "    \n",
    "    # add back in the prev<i> columns\n",
    "    probabilities = pd.merge(probabilities, df_mc[['prev_events'] + prev_cols].drop_duplicates(), on='prev_events', how='left')\n",
    "    probabilities = movecol(probabilities, prev_cols[::-1], 'prev_events', 'Before')\n",
    "    \n",
    "    # normalize the probabilities so each prev_events group sums to 1\n",
    "    probabilities = normalize_probabilities(probabilities, 'probability')\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def get_probability_table(save_path=\"data/probability_tables/probabilities_avg_NOHIT.csv\"):\n",
    "\n",
    "    # compute all of the individual probability tables\n",
    "    df = pd.read_feather('data/play_by_play/play_by_play_full_state_space.feather')\n",
    "    probabilities1 = compute_probabilities(df, 1)\n",
    "    probabilities2 = compute_probabilities(df, 2)\n",
    "    probabilities3 = compute_probabilities(df, 3)\n",
    "    \n",
    "    curr_events = probabilities1['curr_event'].unique()\n",
    "\n",
    "    # create a probabilities dataframe with all of the possible 3-state + curr combinations\n",
    "    start_states = [('#', '#', '#')]\n",
    "    start_states += list(product(['#'], ['#'], curr_events))\n",
    "    start_states += list(product(['#'], curr_events, curr_events))\n",
    "    start_states += list(product(curr_events, curr_events, curr_events))\n",
    "\n",
    "    all_states = list(product(start_states, curr_events))\n",
    "    all_states = [a + (b, ) for a, b in all_states]\n",
    "\n",
    "    probabilities_avg = pd.DataFrame(all_states, columns=['prev3', 'prev2', 'prev1', 'curr_event'])\n",
    "    probabilities_avg['3event'] = probabilities_avg['prev3'] + ',' + probabilities_avg['prev2'] + ',' + probabilities_avg['prev1']\n",
    "    probabilities_avg['2event'] = probabilities_avg['prev2'] + ',' + probabilities_avg['prev1']\n",
    "    probabilities_avg['1event'] = probabilities_avg['prev1']\n",
    "\n",
    "    probabilities_avg['3event_join'] = probabilities_avg['3event'] + '|' + probabilities_avg['curr_event']\n",
    "    probabilities_avg['2event_join'] = probabilities_avg['2event'] + '|' + probabilities_avg['curr_event']\n",
    "    probabilities_avg['1event_join'] = probabilities_avg['1event'] + '|' + probabilities_avg['curr_event']\n",
    "\n",
    "    probabilities1['1event_join'] = probabilities1['prev_events'] + '|' + probabilities1['curr_event']\n",
    "    probabilities2['2event_join'] = probabilities2['prev_events'] + '|' + probabilities2['curr_event']\n",
    "    probabilities3['3event_join'] = probabilities3['prev_events'] + '|' + probabilities3['curr_event']\n",
    "\n",
    "    probabilities_avg = probabilities_avg.merge(probabilities1[['1event_join', 'probability', 'given_count', 'join_count']], on='1event_join', how='left', suffixes=('', '_1'))\n",
    "    probabilities_avg = probabilities_avg.merge(probabilities2[['2event_join', 'probability', 'given_count', 'join_count']], on='2event_join', how='left', suffixes=('', '_2'))\n",
    "    probabilities_avg = probabilities_avg.merge(probabilities3[['3event_join', 'probability', 'given_count', 'join_count']], on='3event_join', how='left', suffixes=('', '_3'))\n",
    "    probabilities_avg = probabilities_avg.fillna(0)\n",
    "    probabilities_avg = probabilities_avg.rename(columns={'given_count': 'given_count_1', 'join_count': 'join_count_1', 'probability': 'probability_1'})\n",
    "\n",
    "    probabilities_avg['probability_avg'] = (probabilities_avg['probability_1'] + probabilities_avg['probability_2'] + probabilities_avg['probability_3']) / 3\n",
    "    rel_cols = [\n",
    "        'prev3', 'prev2', 'prev1', 'curr_event', 'probability_avg',\n",
    "        'probability_1', 'probability_2', 'probability_3',\n",
    "        'given_count_1', 'given_count_2', 'given_count_3',\n",
    "        'join_count_1',  'join_count_2', 'join_count_3']\n",
    "    probabilities_avg = probabilities_avg[rel_cols]\n",
    "\n",
    "    # normalize the probabilities_avg\n",
    "    prob_avg_sum = probabilities_avg.groupby(['prev3', 'prev2', 'prev1'])['probability_avg'].sum().reset_index()\n",
    "    probabilities_avg = pd.merge(probabilities_avg, prob_avg_sum, on=['prev3', 'prev2', 'prev1'], suffixes=('', '_sum'))\n",
    "    probabilities_avg['probability_avg'] = probabilities_avg['probability_avg'] / probabilities_avg['probability_avg_sum']\n",
    "    probabilities_avg = probabilities_avg.drop(columns=['probability_avg_sum'])\n",
    "    \n",
    "    probabilities_avg.reset_index(drop=True).to_csv(save_path, index=False)\n",
    "    \n",
    "    return probabilities_avg\n",
    "\n",
    "probabilities_avg = get_probability_table()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = probabilities_avg.groupby(['prev3', 'prev2', 'prev1'])['probability_avg'].sum().reset_index()\n",
    "bad_probs = np.abs(thing['probability_avg'].values - 1) > 1e-1\n",
    "\n",
    "display(thing.loc[bad_probs, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_events = probabilities1['curr_event'].unique()\n",
    "\n",
    "# create a probabilities dataframe with all of the possible 3-state + curr combinations\n",
    "start_states = [('#', '#', '#')]\n",
    "start_states += list(product(['#'], ['#'], curr_events))\n",
    "start_states += list(product(['#'], curr_events, curr_events))\n",
    "start_states += list(product(curr_events, curr_events, curr_events))\n",
    "\n",
    "all_states = list(product(start_states, curr_events))\n",
    "all_states = [a + (b, ) for a, b in all_states]\n",
    "\n",
    "probabilities_avg = pd.DataFrame(all_states, columns=['prev3', 'prev2', 'prev1', 'curr_event'])\n",
    "probabilities_avg['3event'] = probabilities_avg['prev3'] + ',' + probabilities_avg['prev2'] + ',' + probabilities_avg['prev1']\n",
    "probabilities_avg['2event'] = probabilities_avg['prev2'] + ',' + probabilities_avg['prev1']\n",
    "probabilities_avg['1event'] = probabilities_avg['prev1']\n",
    "\n",
    "probabilities_avg['3event_join'] = probabilities_avg['3event'] + '|' + probabilities_avg['curr_event']\n",
    "probabilities_avg['2event_join'] = probabilities_avg['2event'] + '|' + probabilities_avg['curr_event']\n",
    "probabilities_avg['1event_join'] = probabilities_avg['1event'] + '|' + probabilities_avg['curr_event']\n",
    "\n",
    "probabilities1['1event_join'] = probabilities1['prev_events'] + '|' + probabilities1['curr_event']\n",
    "probabilities2['2event_join'] = probabilities2['prev_events'] + '|' + probabilities2['curr_event']\n",
    "probabilities3['3event_join'] = probabilities3['prev_events'] + '|' + probabilities3['curr_event']\n",
    "\n",
    "probabilities_avg = probabilities_avg.merge(probabilities1[['1event_join', 'probability', 'given_count', 'join_count']], on='1event_join', how='left', suffixes=('', '_1'))\n",
    "probabilities_avg = probabilities_avg.merge(probabilities2[['2event_join', 'probability', 'given_count', 'join_count']], on='2event_join', how='left', suffixes=('', '_2'))\n",
    "probabilities_avg = probabilities_avg.merge(probabilities3[['3event_join', 'probability', 'given_count', 'join_count']], on='3event_join', how='left', suffixes=('', '_3'))\n",
    "probabilities_avg = probabilities_avg.fillna(0)\n",
    "probabilities_avg = probabilities_avg.rename(columns={'given_count': 'given_count_1', 'join_count': 'join_count_1', 'probability': 'probability_1'})\n",
    "\n",
    "probabilities_avg['probability_avg'] = (probabilities_avg['probability_1'] + probabilities_avg['probability_2'] + probabilities_avg['probability_3']) / 3\n",
    "rel_cols = [\n",
    "    'prev3', 'prev2', 'prev1', 'curr_event', 'probability_avg',\n",
    "    'probability_1', 'probability_2', 'probability_3',\n",
    "    'given_count_1', 'given_count_2', 'given_count_3',\n",
    "    'join_count_1',  'join_count_2', 'join_count_3']\n",
    "probabilities_avg = probabilities_avg[rel_cols]\n",
    "\n",
    "# normalize the probabilities_avg\n",
    "prob_avg_sum = probabilities_avg.groupby(['prev3', 'prev2', 'prev1'])['probability_avg'].sum().reset_index()\n",
    "probabilities_avg = pd.merge(probabilities_avg, prob_avg_sum, on=['prev3', 'prev2', 'prev1'], suffixes=('', '_sum'))\n",
    "probabilities_avg['probability_avg'] = probabilities_avg['probability_avg'] / probabilities_avg['probability_avg_sum']\n",
    "probabilities_avg = probabilities_avg.drop(columns=['probability_avg_sum'])\n",
    "\n",
    "display(probabilities_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the event and the game_id\n",
    "df_mc = df_events[['game_id', 'event']].copy()\n",
    "\n",
    "# keep track of the first rows of each game\n",
    "first_rows = df_mc['game_id'] != df_mc['game_id'].shift(1)\n",
    "second_rows = (df_mc['game_id'] != df_mc['game_id'].shift(2)) & (~first_rows)\n",
    "third_rows = (df_mc['game_id'] != df_mc['game_id'].shift(3)) & (~first_rows) & (~second_rows)\n",
    "\n",
    "# find the previous 1,2,3 events and add a placehold \"#\" if there is not prevx event\n",
    "df_mc['prev1'] = df_mc['event'].shift(1)\n",
    "df_mc['prev2'] = df_mc['event'].shift(2)\n",
    "df_mc['prev3'] = df_mc['event'].shift(3)\n",
    "df_mc.loc[first_rows, 'prev1'] = \"#\"\n",
    "df_mc.loc[first_rows | second_rows, 'prev2'] = \"#\"\n",
    "df_mc.loc[first_rows | second_rows | third_rows, 'prev3'] = \"#\"\n",
    "\n",
    "# concatenate the previous events into a 3-state representation\n",
    "df_mc = df_mc.dropna()\n",
    "df_mc['prev_events'] =  df_mc['prev3'] + ',' + df_mc['prev2'] + ',' + df_mc['prev1']\n",
    "df_mc['curr_event'] = df_mc['event']\n",
    "\n",
    "# create a probabilities dataframe that continas the probability of moving from\n",
    "# prev3-prev2-prev1 -> curr_event\n",
    "prev_events = df_mc['prev_events'].unique()\n",
    "curr_events = df_mc['curr_event'].unique()\n",
    "\n",
    "probabilities3 = pd.DataFrame(list(product(prev_events, curr_events)), columns=['prev_events', 'curr_event'])\n",
    "probabilities3['join'] = probabilities3['prev_events'] + '|' + probabilities3['curr_event']\n",
    "\n",
    "# compute the given-this counts\n",
    "given_counts = df_mc['prev_events'].value_counts()\n",
    "given_counts = pd.DataFrame(given_counts).reset_index()\n",
    "given_counts.columns = ['prev_events', 'given_count']\n",
    "\n",
    "# compute the joint counts\n",
    "df_mc['join'] = df_mc['prev_events'] + '|' + df_mc['curr_event']\n",
    "join_counts = df_mc['join'].value_counts()\n",
    "join_counts = pd.DataFrame(join_counts).reset_index()\n",
    "join_counts.columns = ['join', 'join_count']\n",
    "\n",
    "# join the counts onto the probabilities dataframe\n",
    "probabilities3 = probabilities3.merge(given_counts, on='prev_events', how='left')\n",
    "probabilities3 = probabilities3.merge(join_counts, on='join', how='left')\n",
    "probabilities3 = probabilities3.fillna(0)\n",
    "probabilities3['probability'] = probabilities3['join_count'] / probabilities3['given_count']\n",
    "probabilities3 = probabilities3.sort_values(by='probability', ascending=False)\n",
    "probabilities3 = probabilities3[['prev_events', 'curr_event', 'probability', 'given_count', 'join_count']]\n",
    "display(probabilities3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the event and the game_id\n",
    "df_mc = df_events[['game_id', 'event']].copy()\n",
    "\n",
    "# keep track of the first rows of each game\n",
    "first_rows = df_mc['game_id'] != df_mc['game_id'].shift(1)\n",
    "second_rows = (df_mc['game_id'] != df_mc['game_id'].shift(2)) & (~first_rows)\n",
    "\n",
    "# find the previous 1,2,3 events and add a placehold \"#\" if there is not prevx event\n",
    "df_mc['prev1'] = df_mc['event'].shift(1)\n",
    "df_mc['prev2'] = df_mc['event'].shift(2)\n",
    "df_mc.loc[first_rows, 'prev1'] = \"#\"\n",
    "df_mc.loc[first_rows | second_rows, 'prev2'] = \"#\"\n",
    "\n",
    "# concatenate the previous events into a 3-state representation\n",
    "df_mc = df_mc.dropna()\n",
    "df_mc['prev_events'] =  df_mc['prev2'] + ',' + df_mc['prev1']\n",
    "df_mc['curr_event'] = df_mc['event']\n",
    "\n",
    "# create a probabilities dataframe that continas the probability of moving from\n",
    "# prev3-prev2-prev1 -> curr_event\n",
    "prev_events = df_mc['prev_events'].unique()\n",
    "curr_events = df_mc['curr_event'].unique()\n",
    "\n",
    "probabilities2 = pd.DataFrame(list(product(prev_events, curr_events)), columns=['prev_events', 'curr_event'])\n",
    "probabilities2['join'] = probabilities2['prev_events'] + '|' + probabilities2['curr_event']\n",
    "\n",
    "# compute the given-this counts\n",
    "given_counts = df_mc['prev_events'].value_counts()\n",
    "given_counts = pd.DataFrame(given_counts).reset_index()\n",
    "given_counts.columns = ['prev_events', 'given_count']\n",
    "\n",
    "# compute the joint counts\n",
    "df_mc['join'] = df_mc['prev_events'] + '|' + df_mc['curr_event']\n",
    "join_counts = df_mc['join'].value_counts()\n",
    "join_counts = pd.DataFrame(join_counts).reset_index()\n",
    "join_counts.columns = ['join', 'join_count']\n",
    "\n",
    "# join the counts onto the probabilities dataframe\n",
    "probabilities2 = probabilities2.merge(given_counts, on='prev_events', how='left')\n",
    "probabilities2 = probabilities2.merge(join_counts, on='join', how='left')\n",
    "probabilities2 = probabilities2.fillna(0)\n",
    "probabilities2['probability'] = probabilities2['join_count'] / probabilities2['given_count']\n",
    "probabilities2 = probabilities2.sort_values(by='probability', ascending=False)\n",
    "probabilities2 = probabilities2[['prev_events', 'curr_event', 'probability', 'given_count', 'join_count']]\n",
    "display(probabilities2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the event and the game_id\n",
    "df_mc = df_events[['game_id', 'event']].copy()\n",
    "\n",
    "# keep track of the first rows of each game\n",
    "first_rows = df_mc['game_id'] != df_mc['game_id'].shift(1)\n",
    "\n",
    "# find the previous 1,2,3 events and add a placehold \"#\" if there is not prevx event\n",
    "df_mc['prev1'] = df_mc['event'].shift(1)\n",
    "df_mc.loc[first_rows, 'prev1'] = \"#\"\n",
    "\n",
    "# concatenate the previous events into a 3-state representation\n",
    "df_mc = df_mc.dropna()\n",
    "df_mc['prev_events'] =  df_mc['prev1']\n",
    "df_mc['curr_event'] = df_mc['event']\n",
    "\n",
    "# create a probabilities dataframe that continas the probability of moving from\n",
    "# prev3-prev2-prev1 -> curr_event\n",
    "prev_events = df_mc['prev_events'].unique()\n",
    "curr_events = df_mc['curr_event'].unique()\n",
    "\n",
    "probabilities1 = pd.DataFrame(list(product(prev_events, curr_events)), columns=['prev_events', 'curr_event'])\n",
    "probabilities1['join'] = probabilities1['prev_events'] + '|' + probabilities1['curr_event']\n",
    "\n",
    "# compute the given-this counts\n",
    "given_counts = df_mc['prev_events'].value_counts()\n",
    "given_counts = pd.DataFrame(given_counts).reset_index()\n",
    "given_counts.columns = ['prev_events', 'given_count']\n",
    "\n",
    "# compute the joint counts\n",
    "df_mc['join'] = df_mc['prev_events'] + '|' + df_mc['curr_event']\n",
    "join_counts = df_mc['join'].value_counts()\n",
    "join_counts = pd.DataFrame(join_counts).reset_index()\n",
    "join_counts.columns = ['join', 'join_count']\n",
    "\n",
    "# join the counts onto the probabilities dataframe\n",
    "probabilities1 = probabilities1.merge(given_counts, on='prev_events', how='left')\n",
    "probabilities1 = probabilities1.merge(join_counts, on='join', how='left')\n",
    "probabilities1 = probabilities1.fillna(0)\n",
    "probabilities1['probability'] = probabilities1['join_count'] / probabilities1['given_count']\n",
    "probabilities1 = probabilities1.sort_values(by='probability', ascending=False)\n",
    "probabilities1 = probabilities1[['prev_events', 'curr_event', 'probability', 'given_count', 'join_count']]\n",
    "display(probabilities1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities1.to_csv('data/probabilities1_NOHIT.csv', index=False)\n",
    "probabilities2.to_csv('data/probabilities2_NOHIT.csv', index=False)\n",
    "probabilities3.to_csv('data/probabilities3_NOHIT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a probabilities dataframe with all of the possible 3-state + curr combinations\n",
    "start_states = [('#', '#', '#')]\n",
    "start_states += list(product(['#'], ['#'], curr_events))\n",
    "start_states += list(product(['#'], curr_events, curr_events))\n",
    "start_states += list(product(curr_events, curr_events, curr_events))\n",
    "\n",
    "all_states = list(product(start_states, curr_events))\n",
    "all_states = [a + (b, ) for a, b in all_states]\n",
    "\n",
    "probabilities_avg = pd.DataFrame(all_states, columns=['prev3', 'prev2', 'prev1', 'curr_event'])\n",
    "probabilities_avg['3event'] = probabilities_avg['prev3'] + ',' + probabilities_avg['prev2'] + ',' + probabilities_avg['prev1']\n",
    "probabilities_avg['2event'] = probabilities_avg['prev2'] + ',' + probabilities_avg['prev1']\n",
    "probabilities_avg['1event'] = probabilities_avg['prev1']\n",
    "\n",
    "probabilities_avg['3event_join'] = probabilities_avg['3event'] + '|' + probabilities_avg['curr_event']\n",
    "probabilities_avg['2event_join'] = probabilities_avg['2event'] + '|' + probabilities_avg['curr_event']\n",
    "probabilities_avg['1event_join'] = probabilities_avg['1event'] + '|' + probabilities_avg['curr_event']\n",
    "\n",
    "probabilities1['1event_join'] = probabilities1['prev_events'] + '|' + probabilities1['curr_event']\n",
    "probabilities2['2event_join'] = probabilities2['prev_events'] + '|' + probabilities2['curr_event']\n",
    "probabilities3['3event_join'] = probabilities3['prev_events'] + '|' + probabilities3['curr_event']\n",
    "\n",
    "probabilities_avg = probabilities_avg.merge(probabilities1[['1event_join', 'probability', 'given_count', 'join_count']], on='1event_join', how='left', suffixes=('', '_1'))\n",
    "probabilities_avg = probabilities_avg.merge(probabilities2[['2event_join', 'probability', 'given_count', 'join_count']], on='2event_join', how='left', suffixes=('', '_2'))\n",
    "probabilities_avg = probabilities_avg.merge(probabilities3[['3event_join', 'probability', 'given_count', 'join_count']], on='3event_join', how='left', suffixes=('', '_3'))\n",
    "probabilities_avg = probabilities_avg.fillna(0)\n",
    "probabilities_avg = probabilities_avg.rename(columns={'given_count': 'given_count_1', 'join_count': 'join_count_1', 'probability': 'probability_1'})\n",
    "\n",
    "probabilities_avg['probability_avg'] = (probabilities_avg['probability_1'] + probabilities_avg['probability_2'] + probabilities_avg['probability_3']) / 3\n",
    "rel_cols = [\n",
    "    'prev3', 'prev2', 'prev1', 'curr_event', 'probability_avg',\n",
    "    'probability_1', 'probability_2', 'probability_3',\n",
    "    'given_count_1', 'given_count_2', 'given_count_3',\n",
    "    'join_count_1',  'join_count_2', 'join_count_3']\n",
    "probabilities_avg = probabilities_avg[rel_cols]\n",
    "\n",
    "display(probabilities_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_avg.to_csv('data/probabilities_avg_NOHIT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check to make sure probability function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = probabilities1.copy()\n",
    "p1_func = probabilities1_func.copy()\n",
    "p2 = probabilities2.copy()\n",
    "p2_func = probabilities2_func.copy()\n",
    "p3 = probabilities3.copy()\n",
    "p3_func = probabilities3_func.copy()\n",
    "\n",
    "display(p1)\n",
    "display(p1_func)\n",
    "display(p2)\n",
    "display(p2_func)\n",
    "display(p3)\n",
    "display(p3_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = p1.sort_values(by='prev_events').reset_index(drop=True)\n",
    "p1_func = p1_func.sort_values(by='prev_events').reset_index(drop=True)\n",
    "\n",
    "print(np.sum(p1['prev_events'] != p1_func['prev_events']))\n",
    "\n",
    "plt.hist(p1['probability'] - p1_func['probability'], bins=100)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(p1['given_count'] - p1_func['given_count'], bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis / display stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number of times each event occurs\n",
    "print(\"Number of events per type over 10 years\")\n",
    "print(df['event_type'].value_counts())\n",
    "\n",
    "# show the distribution of events per game\n",
    "events_per_game = df.groupby('game_id').count()['event_type']\n",
    "print(\"Events per game\")\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(events_per_game, vert=False)\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(events_per_game, bins=100)\n",
    "plt.gcf().set_size_inches(10, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a correlation matrix between the columns probability_1, probability_2, probability_3\n",
    "corr = probabilities_avg[['probability_1', 'probability_2', 'probability_3']].corr()\n",
    "display(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(probabilities_avg['probability_avg'], label=\"avg\", bins=100)\n",
    "plt.title(\"Average probability\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(probabilities_avg['probability_1'], label=\"1event\", bins=100)\n",
    "plt.title(\"Probability 1event\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(probabilities_avg['probability_2'], label=\"2event\", bins=100)\n",
    "plt.title(\"Probability 2event\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(probabilities_avg['probability_3'], label=\"3event\", bins=100)\n",
    "plt.title(\"Probability 3event\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seconds in game analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_orig[df_orig['game_id'] == 2010020048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Seconds in between events\")\n",
    "df_events['minutes_diff'] = df_events['seconds_diff'] / 60\n",
    "df_events = df_events[df_events['seconds_diff'] < 400]\n",
    "\n",
    "plt.hist(df_events['seconds_diff'], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulate a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs = pd.read_csv(\"data/probabilities_avg.csv\")\n",
    "probs = pd.read_csv(\"data/probabilities_avg_NOHIT.csv\")\n",
    "display(probs)\n",
    "\n",
    "events = probs['curr_event'].unique()\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more or less normalize the probabilities\n",
    "probs_sums = (probs\n",
    "    .copy()\n",
    "    .groupby(by=['prev3', 'prev2', 'prev1'])\n",
    "    [['probability_avg', 'probability_1', 'probability_2', 'probability_3']]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'probability_avg': 'probability_avg_sum',\n",
    "        'probability_1': 'probability_1_sum',\n",
    "        'probability_2': 'probability_2_sum',\n",
    "        'probability_3': 'probability_3_sum'}))\n",
    "\n",
    "display(probs_sums)\n",
    "\n",
    "probs2 = pd.merge(probs, probs_sums, on=['prev3', 'prev2', 'prev1'], how='left')\n",
    "probs2['probability_avg'] /= probs2['probability_avg_sum']\n",
    "probs2['probability_1'] /= probs2['probability_1_sum']\n",
    "probs2['probability_2'] /= probs2['probability_2_sum']\n",
    "probs2['probability_3'] /= probs2['probability_3_sum']\n",
    "probs2 = probs2.drop(columns=[\n",
    "    'probability_avg_sum', 'probability_1_sum', 'probability_2_sum', 'probability_3_sum'])\n",
    "\n",
    "probs2 = probs2.fillna(0)\n",
    "display(probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_distr = df_events['seconds_diff'].to_numpy()\n",
    "\n",
    "# fit a gaussian KernelDensity on seconds_distr\n",
    "# Import the necessary module\n",
    "# Fit a Gaussian KernelDensity on seconds_distr\n",
    "kde = KernelDensity(kernel='gaussian').fit(seconds_distr[:, np.newaxis])\n",
    "\n",
    "# sample 10 times from this kde\n",
    "samples = kde.sample(100000).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/pickles/kde_seconds_NOHIT.pickle\", \"wb\") as file:\n",
    "    pickle.dump(kde, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the frickin game\n",
    "\n",
    "n_games = 1000\n",
    "i = 0\n",
    "\n",
    "games = []\n",
    "\n",
    "game_bar = tqdm(total=n_games)\n",
    "\n",
    "for game_id in range(n_games):\n",
    "    \n",
    "    game_id = str(game_id).zfill(8)\n",
    "    \n",
    "    seconds_remaining = 3600\n",
    "    \n",
    "    prev3 = \"#\"\n",
    "    prev2 = \"#\"\n",
    "    prev1 = \"#\"\n",
    "    \n",
    "    home_score = 0\n",
    "    away_score = 0\n",
    "    \n",
    "    curr_dict = {e:0 for e in events}\n",
    "    curr_dict['time_remaining'] = seconds_remaining\n",
    "    \n",
    "    game_dicts = [curr_dict.copy()]\n",
    "    \n",
    "    while seconds_remaining > 0:\n",
    "        \n",
    "        game_bar.set_description(str(seconds_remaining))\n",
    "        \n",
    "        curr_table = probs2[(probs['prev3'] == prev3) & (probs2['prev2'] == prev2) & (probs2['prev1'] == prev1)]\n",
    "\n",
    "        curr_event = np.random.choice(curr_table['curr_event'], p=curr_table['probability_avg'])\n",
    "        prev3, prev2, prev1 = prev2, prev1, curr_event\n",
    "        \n",
    "        event_time = samples[i]\n",
    "        i += 1\n",
    "        \n",
    "        seconds_remaining -= event_time\n",
    "        \n",
    "        curr_dict['time_remaining'] = seconds_remaining\n",
    "        curr_dict[curr_event] += 1\n",
    "        game_dicts.append(curr_dict.copy())\n",
    "            \n",
    "    game_bar.update(1)\n",
    "            \n",
    "        \n",
    "    game_df = pd.DataFrame(game_dicts)\n",
    "    game_df['game_id'] = game_id\n",
    "    game_df['home_score'] = home_score\n",
    "    game_df['away_score'] = away_score\n",
    "    games.append(game_df)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_full = pd.concat(games).reset_index(drop=True)\n",
    "\n",
    "last_rows = games_full['game_id'].shift(-1) != games_full['game_id']\n",
    "last_rows_df = games_full.loc[last_rows, :].copy()\n",
    "last_rows_df = movecol(last_rows_df, ['GOAL_HOME', 'GOAL_AWAY'], games_full.columns[0], 'Before')\n",
    "display(last_rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking how simulation matches actual games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_feather(\"data/play_by_play/play_by_play_full.feather\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full.copy()\n",
    "df = df.loc[df['event_type'] == 'HIT', :].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "As = ['1','2','3']\n",
    "Bs = [choice(['A','B','C']) for _ in range(10)]\n",
    "rows = list(product(As, Bs))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=['game_id', 'event_type'])\n",
    "df['order'] = df.index.values.copy()\n",
    "df['count'] = 1\n",
    "df['random'] = np.random.randint(-3, 3, len(df))\n",
    "df['random2'] = [choice([True, False]) for _ in range(len(df))]\n",
    "display(df)\n",
    "\n",
    "piv_tab = (pd.pivot_table(df, values='count', index=['game_id', 'order'], columns=['event_type'])\n",
    "           .reset_index()\n",
    "           .fillna(0))\n",
    "\n",
    "#piv_tab_cols = [c for c in piv_tab.columns if c != 'B' and c != 'C']\n",
    "#piv_tab = piv_tab[piv_tab_cols].copy()\n",
    "\n",
    "for col in ['A','B','C']:\n",
    "\n",
    "    # get a cumulative sum\n",
    "    piv_tab[col+\"_cumsum\"] = piv_tab[col].cumsum()\n",
    "\n",
    "    # for the last entries of each game, get the different between cumulative sums\n",
    "    last_rows = piv_tab['game_id'].shift(-1) != piv_tab['game_id']\n",
    "    last_rows_df = piv_tab.loc[last_rows, :].copy()\n",
    "    last_rows_df[col+\"_cumsum\"] = -1*last_rows_df[col+\"_cumsum\"]\n",
    "    last_rows_df[col+\"_cumsum_diff\"] = last_rows_df[col+\"_cumsum\"].diff().shift(-1).fillna(0)\n",
    "\n",
    "    # align cumsum_diff with the original dataframe\n",
    "    piv_tab = pd.merge(piv_tab, last_rows_df[['game_id', 'order', col+\"_cumsum_diff\"]], on=['game_id', 'order'], how='left')\n",
    "    piv_tab[col+\"_offset\"] = piv_tab[col+\"_cumsum_diff\"].fillna(0)\n",
    "    piv_tab[col+\"_offset2\"] = piv_tab[col+\"_offset\"].shift(1).fillna(0)\n",
    "\n",
    "    # compute the adjusted column\n",
    "    piv_tab[col+\"_adjusted\"] = piv_tab[col].copy() + piv_tab[col+\"_offset2\"]\n",
    "\n",
    "    # cumulative sum column'\n",
    "    piv_tab[col+'_total'] = piv_tab[col+\"_adjusted\"].cumsum()\n",
    "\n",
    "    piv_tab = movecol(piv_tab, [col+'_total'], col, 'After')\n",
    "\n",
    "    drop_cols = [col+'_cumsum', col+'_cumsum_diff', col+'_offset', col+'_offset2', col+\"_adjusted\"]\n",
    "    piv_tab = piv_tab.drop(columns=drop_cols)\n",
    "\n",
    "display(piv_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "print(np.sum(df['event_team_type'] == '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full.copy()\n",
    "\n",
    "# ignore all of the HIT events\n",
    "df = df.loc[\n",
    "    (df['event_type'] != 'HIT') &\n",
    "    (df['event_team_type'] != \"-\"), :].copy()\n",
    "df['event_type'] = df['event_type'].astype(str) + '_' + df['event_team_type'].astype(str).str.upper()\n",
    "\n",
    "df = df.sort_values(by=['game_id', 'game_seconds'], ascending=[True, True])\n",
    "\n",
    "# add an order column to ensure that the pandas merge works later\n",
    "df['order'] = np.arange(len(df))\n",
    "\n",
    "# create a smaller dataframe with just the relevant columns for getting the state counts\n",
    "count_cols = ['game_id', 'event_type', 'order']\n",
    "other_cols = [c for c in df.columns if c not in count_cols]\n",
    "df_rel = df[count_cols].copy()\n",
    "df_rel['count'] = 1\n",
    "\n",
    "# pivot the table on the event_type, so that there is a zero for each event\n",
    "# that occured at each timestep and a 0 for all of the others\n",
    "piv_tab = (pd.pivot_table(df_rel, \n",
    "                        values='count', \n",
    "                        index=['game_id', 'order'], \n",
    "                        columns=['event_type'])\n",
    "        .reset_index()\n",
    "        .fillna(0))\n",
    "\n",
    "# create the cumulative counts for all of the new event columns in the pivot table\n",
    "event_cols = [c for c in piv_tab.columns if c != 'game_id' and c != 'order']\n",
    "pbar = tqdm(total=len(event_cols))\n",
    "for col in event_cols:\n",
    "    pbar.set_description(str(col))\n",
    "    \n",
    "    piv_tab[col] = piv_tab[col].astype(int)\n",
    "    \n",
    "    # get a cumulative sum\n",
    "    piv_tab[col+\"_cumsum\"] = piv_tab[col].cumsum()\n",
    "\n",
    "    # for the last entries of each game, get the different between cumulative sums\n",
    "    last_rows = piv_tab['game_id'].shift(-1) != piv_tab['game_id']\n",
    "    last_rows_df = piv_tab.loc[last_rows, :].copy()\n",
    "    last_rows_df[col+\"_cumsum\"] = -1*last_rows_df[col+\"_cumsum\"]\n",
    "    last_rows_df[col+\"_cumsum_diff\"] = last_rows_df[col+\"_cumsum\"].diff().fillna(0)\n",
    "    first_cumsum = last_rows_df.loc[last_rows_df.index[0], col+\"_cumsum\"]\n",
    "    last_rows_df.loc[last_rows_df.index[0], col+\"_cumsum_diff\"] = first_cumsum\n",
    "\n",
    "    # align cumsum_diff with the original dataframe\n",
    "    piv_tab = pd.merge(piv_tab, last_rows_df[['game_id', 'order', col+\"_cumsum_diff\"]], on=['game_id', 'order'], how='left')\n",
    "    piv_tab[col+\"_offset\"] = piv_tab[col+\"_cumsum_diff\"].fillna(0)\n",
    "    piv_tab[col+\"_offset2\"] = piv_tab[col+\"_offset\"].shift(1).fillna(0)\n",
    "\n",
    "    # compute the adjusted column\n",
    "    piv_tab[col+\"_adjusted\"] = piv_tab[col].copy() + piv_tab[col+\"_offset2\"]\n",
    "\n",
    "    # cumulative sum column'\n",
    "    piv_tab[col+'_raw'] = piv_tab[col].copy()\n",
    "    piv_tab[col+'_total'] = piv_tab[col+\"_adjusted\"].cumsum().astype(int)\n",
    "\n",
    "    # only keep the new total column (get rid of the intermediate columns)\n",
    "    piv_tab = movecol(piv_tab, [col+'_total', col+'_raw'], col, 'After')\n",
    "    piv_tab = piv_tab.drop(columns=[col])\n",
    "    piv_tab = piv_tab.rename(columns={col+'_total': col})\n",
    "    drop_cols = [col+'_cumsum', col+'_cumsum_diff', col+'_offset', col+'_offset2', col+\"_adjusted\"]\n",
    "    piv_tab = piv_tab.drop(columns=drop_cols)\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "df = df.merge(piv_tab, on=['game_id', 'order'], how='left') \n",
    "df.reset_index(drop=True).to_feather(\"data/play_by_play/play_by_play_full_counts.feather\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check out simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    pd.read_feather('data/simulated_games/nohit_500.feather'),\n",
    "    pd.read_feather('data/simulated_games/nohit_1000.feather')\n",
    "])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_rows = df['game_id'].shift(-1) != df['game_id']\n",
    "df_last = df.loc[last_rows, :].copy()\n",
    "\n",
    "display(df_last['GOAL_AWAY'].value_counts().sort_index())\n",
    "display(df_last['GOAL_HOME'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df_last['GOAL_HOME'] - df_last['GOAL_AWAY']\n",
    "plt.hist(diff, bins=len(diff.unique()), edgecolor='white', density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_last)\n",
    "df_last.reset_index(drop=True).to_feather('data/simulated_games/lasts.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
